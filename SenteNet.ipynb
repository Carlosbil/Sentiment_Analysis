{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizing the sentences, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this proyect i will develop an AI able to analyze the sentences and describe it.\n",
    "\n",
    "Twitter's \"Sentiment140\" dataset. It contains 1.6 million tagged tweets for sentiment analysis.\n",
    "\n",
    "The columns are as follows:\n",
    "0. The sentiment of the tweet (0 = negative, 2 = neutral, 4 = positive).\n",
    "1. The ID of the tweet\n",
    "2. The date the tweet was sent\n",
    "3. The query (if there is one)\n",
    "4. The user who sent the tweet\n",
    "5. The text of the tweet\n",
    "\n",
    "Let's load and preprocess this dataset. As we only need the sentiment and text columns, we will ignore the other columns. We will also convert the sentiment labels from 0, 2, 4 to 0, 1, 2 respectively for ease of handling.\n",
    "\n",
    "For this dataset, the preprocessing code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define column names\n",
    "col_names = ['sentiment','id','date','query','user','text']\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "if os.path.exists(\"./dataset/preprocessed_data.csv\"):\n",
    "    # Load preprocessed data\n",
    "    data = pd.read_csv(\"./dataset/preprocessed_data.csv\")\n",
    "else:\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"./dataset/training.1600000.processed.noemoticon.csv\", \n",
    "                       header=None, names=col_names, encoding='latin-1')\n",
    "\n",
    "    # Replace sentiment labels\n",
    "    data['sentiment'] = data['sentiment'].replace({0: 0, 2: 1, 4: 2})\n",
    "\n",
    "    # We only need 'sentiment' and 'text' columns\n",
    "    data = data[['sentiment', 'text']]\n",
    "\n",
    "    # Data cleaning and tokenization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Convert text to lowercase and remove punctuation\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        word_tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and lemmatize remaining words\n",
    "        text = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Apply cleaning function to each text in our dataset\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "    # Save preprocessed data to a new CSV file\n",
    "    data.to_csv(\"./dataset/preprocessed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next step is to convert the words in our dataset into numerical vectors that our neural network can understand. There are several ways to do this, but one of the most common and effective is to use a technique called word embeddings.\n",
    "\n",
    "Word embeddings are a way of representing words as vectors in a high-dimensional space, so that words with similar meanings are close to each other in that space. There are several ways to generate word embeddings, but one of the most common is to use a pre-trained model such as Word2Vec or GloVe.\n",
    "\n",
    "In our case, we will use the Keras Tokenizer API to encode our words. First, we will adjust the tokenizer to our text data so that it learns the vocabulary. Then, we will transform our sequences of words into sequences of numbers with the texts_to_sequences method. Finally, we will use the pad_sequences method to make sure that all our sequences have the same length, which is a requirement for feeding the data to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Instantiate a Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the Tokenizer to our text data\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "\n",
    "# Transform words into sequences of numbers\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "\n",
    "# Pad all sequences to have the same length\n",
    "padded_sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis, a commonly used network architecture is the recurrent neural network (RNN), and more specifically, the Long Short-Term Memory (LSTM) variant. LSTMs are useful because they are able to learn long-term dependencies, which is important in text analysis where the meaning of a word may depend on the words preceding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Tamaño del vocabulario (añadimos 1 porque los índices del Tokenizer empiezan en 1)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Longitud de las secuencias de entrada\n",
    "input_length = len(padded_sequences[0])\n",
    "\n",
    "# Dimensión del espacio de embedding\n",
    "embedding_dim = 50\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # Three units for three classes\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  999/40000 [..............................] - ETA: 2:59:40 - loss: 0.4396 - accuracy: 0.7933\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1999/40000 [>.............................] - ETA: 2:55:26 - loss: 0.4389 - accuracy: 0.7935\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 2999/40000 [=>............................] - ETA: 2:50:47 - loss: 0.4378 - accuracy: 0.7941\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 3999/40000 [=>............................] - ETA: 2:46:10 - loss: 0.4380 - accuracy: 0.7939\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 4999/40000 [==>...........................] - ETA: 2:41:32 - loss: 0.4372 - accuracy: 0.7946\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 5999/40000 [===>..........................] - ETA: 2:36:56 - loss: 0.4352 - accuracy: 0.7961\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 6999/40000 [====>.........................] - ETA: 2:32:54 - loss: 0.4337 - accuracy: 0.7970\n",
      "Epoch 1: saving model to .\\best_model.hdf5\n",
      " 7615/40000 [====>.........................] - ETA: 2:32:10 - loss: 0.4331 - accuracy: 0.7978"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7633/40000 [====>.........................] - ETA: 2:32:09 - loss: 0.4331 - accuracy: 0.7978"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "# Define where to save or load the model\n",
    "filepath = \"./best_model.hdf5\"\n",
    "\n",
    "# If the checkpoint file exists, load the model from the checkpoint\n",
    "if os.path.exists(filepath):\n",
    "    model = load_model(filepath)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, mode='min', save_freq=1000)\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels = to_categorical(data['sentiment'])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(padded_sequences, labels, epochs=10, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model \u001b[39m=\u001b[39m load_model(filepath)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Convert labels to one-hot encoding\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m labels \u001b[39m=\u001b[39m to_categorical(data[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[39m# Evaluate the model on the data subset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(padded_sequences, labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "filepath = \"./best_model.hdf5\"\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    model = load_model(filepath)\n",
    "# Convert labels to one-hot encoding\n",
    "labels = to_categorical(data['sentiment'])\n",
    "# Evaluate the model on the data subset\n",
    "loss, accuracy = model.evaluate(padded_sequences, labels)\n",
    "\n",
    "# Print the loss and accuracy\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
